#!/usr/bin/env python3
"""
åŠ è½½GRPOè®­ç»ƒåçš„æ¨¡å‹ç”¨äºæµ‹è¯•
"""

import os
import torch
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM
from trl import AutoModelForCausalLMWithValueHead
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from akgr.abduction_model.diffusion import Diffusion
from akgr.abduction_model.transformer import create_transformer
from akgr.utils.load_util import load_yaml
from akgr.tokenizer import create_tokenizer
from akgr.dataloader import new_create_dataloader, new_create_dataset
from akgr.kgdata import load_kg

def load_grpo_model(checkpoint_path, device='cuda'):
    """
    åŠ è½½GRPOè®­ç»ƒåçš„æ¨¡å‹ - ä¸“ä¸ºGPT2æ ¼å¼ä¼˜åŒ–
    
    Args:
        checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        device: è®¾å¤‡
    
    Returns:
        state_dict: ç›´æ¥è¿”å›state_dictï¼ˆå› ä¸ºç¡®ä¿æ˜¯GPT2æ ¼å¼ï¼‰
        tokenizer: tokenizer
    """
    print(f"Loading model from: {checkpoint_path}")
    
    # æ£€æŸ¥æ£€æŸ¥ç‚¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
    print(f"Tokenizer loaded with vocab size: {tokenizer.vocab_size}")
    
    # ç›´æ¥åŠ è½½æƒé‡æ–‡ä»¶
    state_dict_path = os.path.join(checkpoint_path, 'pytorch_model.bin')
    if not os.path.exists(state_dict_path):
        raise FileNotFoundError(f"pytorch_model.bin not found in: {checkpoint_path}")
    
    print(f"Loading state dict from: {state_dict_path}")
    state_dict = torch.load(state_dict_path, map_location=device)
    print(f"State dict loaded with {len(state_dict)} parameters")
    
    # éªŒè¯æ˜¯GPT2æ ¼å¼
    gpt2_keys = [k for k in state_dict.keys() if 'transformer.' in k]
    opt_keys = [k for k in state_dict.keys() if 'decoder.' in k]
    
    print(f"Found {len(gpt2_keys)} GPT2-style keys, {len(opt_keys)} OPT-style keys")
    
    # æ˜¾ç¤ºå…³é”®æƒé‡ä¿¡æ¯
    vocab_keys = []
    for key in state_dict.keys():
        if any(word in key.lower() for word in ['embed', 'wte', 'lm_head']):
            vocab_keys.append(key)
            print(f"  ğŸ“Š {key}: {state_dict[key].shape}")
    
    if gpt2_keys:
        print("âœ… Confirmed GPT2 format - returning state dict")
    else:
        print("âš ï¸  Warning: No GPT2-style keys found, but proceeding with state dict")
    
    return state_dict, tokenizer

def create_diffusion_model_with_loaded_weights(checkpoint_path, device='cuda'):
    """
    åˆ›å»ºDiffusionæ¨¡å‹å¹¶åŠ è½½GRPOè®­ç»ƒçš„æƒé‡
    
    Args:
        checkpoint_path: GRPOæ£€æŸ¥ç‚¹è·¯å¾„
        device: è®¾å¤‡
    
    Returns:
        model: åŠ è½½äº†æƒé‡çš„Diffusionæ¨¡å‹
        tokenizer: tokenizer
    """
    print(f"Creating Diffusion model and loading weights from: {checkpoint_path}")
    
    # åŠ è½½é…ç½®
    config_dataloader = load_yaml('akgr/configs/config-dataloader.yml')
    config_model = load_yaml('akgr/configs/config-model.yml')
    
    # åŠ è½½çŸ¥è¯†å›¾è°±æ•°æ®
    data_root = config_dataloader['data_root']
    dataname = config_dataloader['dataname']
    scale = config_dataloader['scale']
    max_answer_size = config_dataloader['max_answer_size']
    
    # åŠ è½½çŸ¥è¯†å›¾è°±
    graph_samplers, nentity, nrelation, offset, special_tokens = load_kg(
        data_root=data_root,
        dataname=dataname,
        scale=scale,
        max_answer_size=max_answer_size
    )
    
    # åˆ›å»ºtokenizer
    tokenizer, ntoken = create_tokenizer(
        special_tokens=special_tokens,
        offset=offset,
        nentity=nentity,
        nrelation=nrelation,
        is_gpt=True  # å‡è®¾æ˜¯GPTæ¨¡å‹
    )
    
    # åˆ›å»ºDiffusionæ¨¡å‹
    model = Diffusion(
        ntoken=ntoken,
        special_tokens=special_tokens,
        model_name='gpt2',  # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        config_model=config_model,
        device=device,
        drop=0.0,
        generation_config=None
    )
    
    # åŠ è½½GRPOè®­ç»ƒçš„æƒé‡
    print("Loading GRPO weights...")
    grpo_model, _ = load_grpo_model(checkpoint_path, device)
    
    # å°†GRPOæ¨¡å‹çš„æƒé‡å¤åˆ¶åˆ°Diffusionæ¨¡å‹ä¸­
    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ¨¡å‹ç»“æ„è¿›è¡Œè°ƒæ•´
    if hasattr(grpo_model, 'pretrained_model'):
        # TRLæ¨¡å‹çš„æƒ…å†µ
        grpo_state_dict = grpo_model.pretrained_model.state_dict()
    else:
        # æ ‡å‡†æ¨¡å‹çš„æƒ…å†µ
        grpo_state_dict = grpo_model.state_dict()
    
    # è¿‡æ»¤æ‰ä¸åŒ¹é…çš„é”®
    model_state_dict = model.model.state_dict()
    filtered_state_dict = {}
    
    for key, value in grpo_state_dict.items():
        if key in model_state_dict and model_state_dict[key].shape == value.shape:
            filtered_state_dict[key] = value
        else:
            print(f"Skipping key {key} due to shape mismatch or missing key")
    
    # åŠ è½½æƒé‡
    missing_keys, unexpected_keys = model.model.load_state_dict(filtered_state_dict, strict=False)
    print(f"Missing keys: {len(missing_keys)}")
    print(f"Unexpected keys: {len(unexpected_keys)}")
    
    model.to(device)
    model.eval()
    
    return model, tokenizer, graph_samplers

def main():
    parser = argparse.ArgumentParser(description='Load GRPO trained model for testing')
    parser.add_argument('--checkpoint_path', type=str, required=True,
                       help='Path to the GRPO checkpoint')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to load model on')
    parser.add_argument('--mode', type=str, default='direct',
                       choices=['direct', 'diffusion'],
                       help='Loading mode: direct or diffusion')
    
    args = parser.parse_args()
    
    if args.mode == 'direct':
        # ç›´æ¥åŠ è½½GRPOæ¨¡å‹
        model, tokenizer = load_grpo_model(args.checkpoint_path, args.device)
        print("Model loaded successfully for direct use")
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
        print("Model ready for testing!")
        
    elif args.mode == 'diffusion':
        # åˆ›å»ºDiffusionæ¨¡å‹å¹¶åŠ è½½æƒé‡
        model, tokenizer, graph_samplers = create_diffusion_model_with_loaded_weights(
            args.checkpoint_path, args.device
        )
        print("Diffusion model loaded successfully with GRPO weights")
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
        print("Model ready for testing!")

if __name__ == "__main__":
    main() 